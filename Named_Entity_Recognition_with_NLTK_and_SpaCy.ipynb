{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Named Entity Recognition with NLTK and SpaCy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOoLk9AMF1h7r603BbYwaIW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiWLoz48-6pg"
      },
      "source": [
        "**Named-entity recognition (NER)** is the process of automatically identifying the entities discussed in a text and classifying them into pre-defined categories such as ‘person’, ‘organization’, ‘location’ and so on. \n",
        "\n",
        "A Named Entity Recognizer is a model that can do this recognizing task. It should be able to identify named entities like ‘America’ , ‘Emily’ , ‘London’ ,etc.. and categorize them as PERSON, LOCATION , and so on. It is a very useful tool and helps in Information Retrival.\n",
        "\n",
        "\n",
        "The **spaCy library** allows you to train NER models by both updating an existing spacy model to suit the specific context of your text documents and also to train a fresh NER model from scratch\n",
        "\n",
        "In spacy, Named Entity Recognition is implemented by the pipeline component ner. Most of the models have it in their processing pipeline by default."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg6KYtS47cnI"
      },
      "source": [
        "**NER with spacy (nothing customised yet !!)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y-1jKI5dbX6"
      },
      "source": [
        "#using spacy\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp=en_core_web_sm.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_YagaH_imbX"
      },
      "source": [
        "doc=nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYUR4RUzjP8r",
        "outputId": "7e964eb8-f1a0-458a-c613-1db68529c68b"
      },
      "source": [
        "print(type(doc))\n",
        "for x in doc.ents:\n",
        "  print(x) #displaying all entities found in doc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'spacy.tokens.doc.Doc'>\n",
            "European\n",
            "Google\n",
            "$5.1 billion\n",
            "Wednesday\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw0tva_Tjb55",
        "outputId": "3ee33c6b-7108-4e61-99aa-039b42f232e9"
      },
      "source": [
        "[(x.text,x.label,x.label_) for x in doc.ents] #displaying text and associated label\n",
        "\n",
        "#European is NORD (nationalities or religious or political groups), Google is an organization, $5.1 billion is monetary value and Wednesday is a date object. They are all correct."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('European', 381, 'NORP'),\n",
              " ('Google', 383, 'ORG'),\n",
              " ('$5.1 billion', 394, 'MONEY'),\n",
              " ('Wednesday', 391, 'DATE')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDvZSTOD7mZB"
      },
      "source": [
        "**visualising results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "ZuRXiJYvmv3_",
        "outputId": "ae136fe6-98c3-4789-9170-2aac2e82fc91"
      },
      "source": [
        "# to display results\n",
        "from spacy import displacy\n",
        "displacy.render(doc, style='ent', jupyter=True, options={'distance': 90}) #visualiing results\n",
        "\n",
        "print(\"\\n NORP \", spacy.explain(\"NORP\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    European\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " authorities fined \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Google\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " a record \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $5.1 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              " on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Wednesday\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " for abusing its power in the mobile phone market and ordered the company to alter its practices</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " NORP  Nationalities or religious or political groups\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl7gA9Hpg2r_",
        "outputId": "bae306bb-f8bd-445d-f184-8607669a6c98"
      },
      "source": [
        "#name entity can be person, organisation , quantity,date etc\n",
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"by default pipeline is \",nlp.pipe_names)\n",
        "#In case your model does not have , you can add it using nlp.add_pipe() method.\n",
        "\n",
        "doc=nlp(\"Australia wants Facebook and Google to pay media companies for news\")\n",
        "\n",
        "print(\"\\ntype(doc) \",type(doc))\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(\"\\n  ent :{} ---> ent.label_ : {} end.start_char: {}  end.end_char: {} \".format( ent ,ent.label_ , ent.start_char, ent.end_char))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "by default pipeline is  ['tagger', 'parser', 'ner']\n",
            "\n",
            "type(doc)  <class 'spacy.tokens.doc.Doc'>\n",
            "\n",
            "  ent :Australia ---> ent.label_ : GPE end.start_char: 0  end.end_char: 9 \n",
            "\n",
            "  ent :Facebook and Google ---> ent.label_ : ORG end.start_char: 16  end.end_char: 35 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CHnJtow7s_I"
      },
      "source": [
        "**need of custom NER!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvsgX-I-92Nj",
        "outputId": "f3025530-bb39-4edf-8bcf-628e3da6fef9"
      },
      "source": [
        "import spacy\n",
        "import en_core_web_sm\n",
        "\n",
        "nlp=en_core_web_sm.load()\n",
        "doc=nlp('tennis champion Emerson was expected to win Wimbeldon')\n",
        "print(doc.ents)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)  #wimbeldon is wrongly classified as PERSON , we need custom NER in such cases"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Emerson, Wimbeldon)\n",
            "Emerson PERSON\n",
            "Wimbeldon PERSON\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfmZCaTnkuCW",
        "outputId": "eb07255d-6aad-4be9-b0f3-d391f62d87de"
      },
      "source": [
        "#in cases like below, we need a  custom entity id  recognition . This is financial specific document and NER is not done properly\n",
        "doc=nlp(\"I donot have money to pay my credit card account\")\n",
        "\n",
        "print(\"\\ntype(doc) \",type(doc))\n",
        "\n",
        "print(len(doc.ents))\n",
        "\n",
        "for ent in doc.ents:\n",
        "  print(\"\\n  ent :{} ---> ent.label_ : {} end.start_char: {}  end.end_char: {} \".format( ent ,ent.label_ , ent.start_char, ent.end_char))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "type(doc)  <class 'spacy.tokens.doc.Doc'>\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkwon6Hw8HSh"
      },
      "source": [
        "**Updating the Named Entity Recognizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HurX0DfZsJxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "407668f5-9afd-4037-be14-201ce6a923e3"
      },
      "source": [
        "#To do this, let’s use an existing pre-trained spacy model and update it with newer examples.\n",
        "\n",
        "#First , let’s load a pre-existing spacy model with an in-built ner component. Then, get the Named Entity Recognizer using get_pipe() method .\n",
        "\n",
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "ner=nlp.get_pipe('ner')\n",
        "print(\"\\n existing ner.labels :\",ner.labels)\n",
        "print(\"\\n len(ner.labels) :\",len(ner.labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " existing ner.labels : ('CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART')\n",
            "\n",
            " len(ner.labels) : 18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZDvSZu18TMb"
      },
      "source": [
        "**Format of the training examples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb70pRY4yo8E"
      },
      "source": [
        "#To update a pretrained model with new examples, you’ll have to provide many examples to meaningfully improve the system\n",
        "\n",
        "#spaCy accepts training data as list of tuples.\n",
        "\n",
        "#Each tuple should contain the text and a dictionary. The dictionary should hold the start and end indices of the named enity in the text, and the category or label of the named entity.\n",
        "\n",
        "#For example, (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p24HKg08qSV0"
      },
      "source": [
        "TRAIN_DATA = [\n",
        "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
        "              (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
        "              (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
        "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
        "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
        "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
        "              (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
        "              (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "              (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
        "              (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
        "              (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
        "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
        "              (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
        "              (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "              (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
        "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
        "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
        "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
        "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
        "              ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZAQTwdF4F8H",
        "outputId": "ff3e99f7-d139-464d-ad5e-7216354d7b41"
      },
      "source": [
        "for _, annotations in TRAIN_DATA:\n",
        "  print(_ ,\" and annotation  is : \", annotations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Walmart is a leading e-commerce company  and annotation  is :  {'entities': [(0, 7, 'ORG')]}\n",
            "I reached Chennai yesterday.  and annotation  is :  {'entities': [(19, 28, 'GPE')]}\n",
            "I recently ordered a book from Amazon  and annotation  is :  {'entities': [(24, 32, 'ORG')]}\n",
            "I was driving a BMW  and annotation  is :  {'entities': [(16, 19, 'PRODUCT')]}\n",
            "I ordered this from ShopClues  and annotation  is :  {'entities': [(20, 29, 'ORG')]}\n",
            "Fridge can be ordered in Amazon   and annotation  is :  {'entities': [(0, 6, 'PRODUCT')]}\n",
            "I bought a new Washer  and annotation  is :  {'entities': [(16, 22, 'PRODUCT')]}\n",
            "I bought a old table  and annotation  is :  {'entities': [(16, 21, 'PRODUCT')]}\n",
            "I bought a fancy dress  and annotation  is :  {'entities': [(18, 23, 'PRODUCT')]}\n",
            "I rented a camera  and annotation  is :  {'entities': [(12, 18, 'PRODUCT')]}\n",
            "I rented a tent for our trip  and annotation  is :  {'entities': [(12, 16, 'PRODUCT')]}\n",
            "I rented a screwdriver from our neighbour  and annotation  is :  {'entities': [(12, 22, 'PRODUCT')]}\n",
            "I repaired my computer  and annotation  is :  {'entities': [(15, 23, 'PRODUCT')]}\n",
            "I got my clock fixed  and annotation  is :  {'entities': [(16, 21, 'PRODUCT')]}\n",
            "I got my truck fixed  and annotation  is :  {'entities': [(16, 21, 'PRODUCT')]}\n",
            "Flipkart started it's journey from zero  and annotation  is :  {'entities': [(0, 8, 'ORG')]}\n",
            "I recently ordered from Max  and annotation  is :  {'entities': [(24, 27, 'ORG')]}\n",
            "Flipkart is recognized as leader in market  and annotation  is :  {'entities': [(0, 8, 'ORG')]}\n",
            "I recently ordered from Swiggy  and annotation  is :  {'entities': [(24, 29, 'ORG')]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEyXtNbc4l9K",
        "outputId": "85c656a8-9e57-48ac-900f-a27e900ab5ee"
      },
      "source": [
        "\"\"\"\n",
        "The above code clearly shows you the training format. You have to add these labels to the ner using ner.add_label() method of pipeline . Below code demonstrates the same\n",
        "\"\"\"\n",
        "for _, annotations in TRAIN_DATA:\n",
        "   for ent in annotations.get(\"entities\"):\n",
        "      print(\"adding label to ner : \" ,ent[2])\n",
        "      ner.add_label(ent[2])\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adding label to ner :  ORG\n",
            "adding label to ner :  GPE\n",
            "adding label to ner :  ORG\n",
            "adding label to ner :  PRODUCT\n",
            "adding label to ner :  ORG\n",
            "adding label to ner :  PRODUCT\n",
            "adding label to ner :  PRODUCT\n",
            "adding label to ner :  PRODUCT\n",
            "adding label to ner :  PRODUCT\n",
            "adding label to ner :  PRODUCT\n",
            "adding label to ner :  PRODUCT\n",
            "adding label to ner :  PRODUCT\n",
            "adding label to ner :  PRODUCT\n",
            "adding label to ner :  PRODUCT\n",
            "adding label to ner :  PRODUCT\n",
            "adding label to ner :  ORG\n",
            "adding label to ner :  ORG\n",
            "adding label to ner :  ORG\n",
            "adding label to ner :  ORG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-DorxuZWHi6",
        "outputId": "c20ae4ff-1587-4a09-b19b-186fbb82e445"
      },
      "source": [
        "print(\"\\n existing ner.labels :\",ner.labels)\n",
        "print(\"\\n len(ner.labels) :\",len(ner.labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " existing ner.labels : ('CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART')\n",
            "\n",
            " len(ner.labels) : 18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0x6qaTeH3X-"
      },
      "source": [
        "\"\"\"\n",
        "Now it’s time to train the NER over these examples. But before you train, remember that apart from ner , the model has other pipeline components. These components should not get affected in training.\n",
        "\n",
        "So, disable the other pipeline components through nlp.disable_pipes() method.\n",
        "\"\"\"\n",
        "\n",
        "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "unaffected_pipes= [ pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_EBDPlK8_AC"
      },
      "source": [
        "**Training the NER model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2bjXspmRqBB"
      },
      "source": [
        "#all of the training is done within the context of the nlp model with disabled pipeline, to prevent the other components from being involved.\n",
        "\n",
        "#Training the NER model\n",
        "\n",
        "#To train an ner model, the model has to be looped over the example for sufficient number of iterations\n",
        "\n",
        "#Before every iteration it’s a good practice to shuffle the examples randomly throughrandom.shuffle() functio\n",
        "\n",
        "#The training data is usually passed in batches.\n",
        "\n",
        "#You can call the minibatch() function of spaCy over the training data that will return you data in batches . \n",
        "  #The minibatch function takes size parameter to denote the batch size. You can make use of the utility function compounding to generate an infinite series of compounding values.\n",
        "\n",
        "#compunding() function takes three inputs which are start ( the first integer value) ,stop (the maximum value that can be generated) and finally compound. \n",
        "  #This value stored in compund is the compounding factor for the series.If you are not clear, check out this link for understanding.\n",
        "\n",
        "#For each iteration , the model or ner is updated through the nlp.update() command. Parameters of nlp.update() are :\n",
        "  #docs: This expects a batch of texts as input. You can pass each batch to the zip method, which will return you batches of text and annotations. `  \n",
        "\n",
        "#At each word, the update() it makes a prediction. It then consults the annotations to check if the prediction is right. If it isn’t , it adjusts the weights so that the correct action will score higher next time.  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hEn-pX6WLtW",
        "outputId": "08bfb093-9675-4ad8-c735-1d7c4a513ddd"
      },
      "source": [
        "#making predictions before training\n",
        "\n",
        "doc=nlp(\"I was driving a Alto\")\n",
        "print(\"\\n entities ,\",[ (ent.text , ent.label_) for ent in doc.ents])\n",
        "#below results are wrong"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " entities , [('Alto', 'LOC')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vqSYhSK98ad"
      },
      "source": [
        "**training and analysing results after one iteration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7G1da2pWnHj",
        "outputId": "8ce9ee34-3729-46c6-cdd0-a44139d9cce5"
      },
      "source": [
        "#training with just one iteration, here details of same\n",
        "\n",
        "from spacy.util import minibatch, compounding\n",
        "import random\n",
        "\n",
        "print(\"len(TRAIN_DATA)\",len(TRAIN_DATA))\n",
        "with nlp.disable_pipes(*unaffected_pipes):\n",
        "  for iteration in range(1): # we are having 30 iteration \n",
        "      random.shuffle(TRAIN_DATA)# randomly shuffling before every iteration\n",
        "      losses={}\n",
        "\n",
        "      batches= minibatch(TRAIN_DATA,size=compounding(4.0, 32.0, 1.001))\n",
        "\n",
        "      for c,batch in enumerate(batches):\n",
        "         print(\"\\nbatch no {} length is {}\".format(c,len(batch)))\n",
        "         print(\"contents of batch {} are:\\n {}\".format(c,batch))\n",
        "\n",
        "         text,annotation=zip(*batch)\n",
        "         nlp.update(text,annotation,drop=0.5,losses=losses) \n",
        "         print(\"\\nloss is {}\",losses)\n",
        "         \n",
        "#Loop over the examples and call nlp.update, which steps through the words of the input.\n",
        "#At each word, update makes a prediction. \n",
        "#It then consults the annotations, to see whether it was right. \n",
        "#If it was wrong, it adjusts its weights so that the correct action will score higher next time\n",
        "\n",
        "#Finally, all of the training is done within the context of the nlp model with disabled pipeline, to prevent the other components from being involved.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(TRAIN_DATA) 19\n",
            "\n",
            "batch no 0 length is 4\n",
            "contents of batch 0 are:\n",
            " [('I got my clock fixed', {'entities': [(16, 21, 'PRODUCT')]}), ('I rented a camera', {'entities': [(12, 18, 'PRODUCT')]}), ('I bought a new Washer', {'entities': [(16, 22, 'PRODUCT')]}), ('Flipkart is recognized as leader in market', {'entities': [(0, 8, 'ORG')]})]\n",
            "\n",
            "loss is {} {'ner': 1.4224304639210459}\n",
            "\n",
            "batch no 1 length is 4\n",
            "contents of batch 1 are:\n",
            " [('I bought a fancy dress', {'entities': [(18, 23, 'PRODUCT')]}), ('I rented a tent for our trip', {'entities': [(12, 16, 'PRODUCT')]}), ('Fridge can be ordered in Amazon ', {'entities': [(0, 6, 'PRODUCT')]}), ('I repaired my computer', {'entities': [(15, 23, 'PRODUCT')]})]\n",
            "\n",
            "loss is {} {'ner': 6.491001476853853}\n",
            "\n",
            "batch no 2 length is 4\n",
            "contents of batch 2 are:\n",
            " [('Walmart is a leading e-commerce company', {'entities': [(0, 7, 'ORG')]}), ('I recently ordered from Max', {'entities': [(24, 27, 'ORG')]}), ('I was driving a BMW', {'entities': [(16, 19, 'PRODUCT')]}), (\"Flipkart started it's journey from zero\", {'entities': [(0, 8, 'ORG')]})]\n",
            "\n",
            "loss is {} {'ner': 12.04315576063891}\n",
            "\n",
            "batch no 3 length is 4\n",
            "contents of batch 3 are:\n",
            " [('I recently ordered a book from Amazon', {'entities': [(24, 32, 'ORG')]}), ('I bought a old table', {'entities': [(16, 21, 'PRODUCT')]}), ('I recently ordered from Swiggy', {'entities': [(24, 29, 'ORG')]}), ('I got my truck fixed', {'entities': [(16, 21, 'PRODUCT')]})]\n",
            "\n",
            "loss is {} {'ner': 12.104766281570846}\n",
            "\n",
            "batch no 4 length is 3\n",
            "contents of batch 4 are:\n",
            " [('I rented a screwdriver from our neighbour', {'entities': [(12, 22, 'PRODUCT')]}), ('I ordered this from ShopClues', {'entities': [(20, 29, 'ORG')]}), ('I reached Chennai yesterday.', {'entities': [(19, 28, 'GPE')]})]\n",
            "\n",
            "loss is {} {'ner': 16.846672738336565}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_CdDcNL-oI8"
      },
      "source": [
        "**#training with  30 iteration.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DalpTNKdi9ag",
        "outputId": "5f1f9867-95ab-48e5-a206-d12f09b75c60"
      },
      "source": [
        "#training with  30 iteration.\n",
        "\n",
        "from spacy.util import minibatch, compounding\n",
        "import random\n",
        "\n",
        "print(\"len(TRAIN_DATA)\",len(TRAIN_DATA))\n",
        "with nlp.disable_pipes(*unaffected_pipes):\n",
        "  for iteration in range(30): # we are having 30 iteration \n",
        "      random.shuffle(TRAIN_DATA)# randomly shuffling before every iteration\n",
        "      losses={}\n",
        "\n",
        "      batches= minibatch(TRAIN_DATA,size=compounding(4.0, 32.0, 1.001))#  # batch up the examples using spaCy's minibatch\n",
        "\n",
        "      for batch in batches:\n",
        "         text,annotation=zip(*batch)\n",
        "         nlp.update(text,annotation,drop=0.5,losses=losses) \n",
        "         print(\"\\n iteration {} , loss is {}\".format(iteration,losses))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(TRAIN_DATA) 19\n",
            "\n",
            " iteration 0 , loss is {'ner': 3.576853639235196}\n",
            "\n",
            " iteration 0 , loss is {'ner': 3.9724964438864845}\n",
            "\n",
            " iteration 0 , loss is {'ner': 6.747934089646151}\n",
            "\n",
            " iteration 0 , loss is {'ner': 14.488696412433683}\n",
            "\n",
            " iteration 0 , loss is {'ner': 18.688489220092606}\n",
            "\n",
            " iteration 1 , loss is {'ner': 4.0160229475659435}\n",
            "\n",
            " iteration 1 , loss is {'ner': 5.3159096537253845}\n",
            "\n",
            " iteration 1 , loss is {'ner': 7.026368742975379}\n",
            "\n",
            " iteration 1 , loss is {'ner': 9.305351496607969}\n",
            "\n",
            " iteration 1 , loss is {'ner': 9.314088947620647}\n",
            "\n",
            " iteration 2 , loss is {'ner': 3.814085906094988}\n",
            "\n",
            " iteration 2 , loss is {'ner': 3.8918759282462005}\n",
            "\n",
            " iteration 2 , loss is {'ner': 5.111412194477644}\n",
            "\n",
            " iteration 2 , loss is {'ner': 8.204993931065708}\n",
            "\n",
            " iteration 2 , loss is {'ner': 10.491811769773022}\n",
            "\n",
            " iteration 3 , loss is {'ner': 1.2740659473674896}\n",
            "\n",
            " iteration 3 , loss is {'ner': 3.2785224414822522}\n",
            "\n",
            " iteration 3 , loss is {'ner': 4.759598728817579}\n",
            "\n",
            " iteration 3 , loss is {'ner': 10.581819293986698}\n",
            "\n",
            " iteration 3 , loss is {'ner': 11.813090364414307}\n",
            "\n",
            " iteration 4 , loss is {'ner': 1.9559401259248261}\n",
            "\n",
            " iteration 4 , loss is {'ner': 4.44112729287815}\n",
            "\n",
            " iteration 4 , loss is {'ner': 4.665185274105966}\n",
            "\n",
            " iteration 4 , loss is {'ner': 4.744547492863489}\n",
            "\n",
            " iteration 4 , loss is {'ner': 9.268485691564706}\n",
            "\n",
            " iteration 5 , loss is {'ner': 0.25923713426527684}\n",
            "\n",
            " iteration 5 , loss is {'ner': 4.090800229660999}\n",
            "\n",
            " iteration 5 , loss is {'ner': 5.352769803585943}\n",
            "\n",
            " iteration 5 , loss is {'ner': 7.549723484057722}\n",
            "\n",
            " iteration 5 , loss is {'ner': 8.647251435059843}\n",
            "\n",
            " iteration 6 , loss is {'ner': 0.40833212609868497}\n",
            "\n",
            " iteration 6 , loss is {'ner': 1.9701948628321588}\n",
            "\n",
            " iteration 6 , loss is {'ner': 3.1860121503964365}\n",
            "\n",
            " iteration 6 , loss is {'ner': 4.834638536831278}\n",
            "\n",
            " iteration 6 , loss is {'ner': 5.249177913443134}\n",
            "\n",
            " iteration 7 , loss is {'ner': 1.4924799968525804}\n",
            "\n",
            " iteration 7 , loss is {'ner': 1.4929543500411455}\n",
            "\n",
            " iteration 7 , loss is {'ner': 4.416561175459265}\n",
            "\n",
            " iteration 7 , loss is {'ner': 6.015489831191644}\n",
            "\n",
            " iteration 7 , loss is {'ner': 7.605473854035836}\n",
            "\n",
            " iteration 8 , loss is {'ner': 3.260968082568433}\n",
            "\n",
            " iteration 8 , loss is {'ner': 3.2633578038535456}\n",
            "\n",
            " iteration 8 , loss is {'ner': 4.0821424579131875}\n",
            "\n",
            " iteration 8 , loss is {'ner': 6.038619782130491}\n",
            "\n",
            " iteration 8 , loss is {'ner': 6.093423248882255}\n",
            "\n",
            " iteration 9 , loss is {'ner': 1.2062248606407842}\n",
            "\n",
            " iteration 9 , loss is {'ner': 3.018478122897914}\n",
            "\n",
            " iteration 9 , loss is {'ner': 5.717926019137376}\n",
            "\n",
            " iteration 9 , loss is {'ner': 9.778358256218251}\n",
            "\n",
            " iteration 9 , loss is {'ner': 9.78006696286415}\n",
            "\n",
            " iteration 10 , loss is {'ner': 1.5185290241633667}\n",
            "\n",
            " iteration 10 , loss is {'ner': 2.1250041830216118}\n",
            "\n",
            " iteration 10 , loss is {'ner': 2.1256856858866753}\n",
            "\n",
            " iteration 10 , loss is {'ner': 6.149803897858834}\n",
            "\n",
            " iteration 10 , loss is {'ner': 7.610650710257704}\n",
            "\n",
            " iteration 11 , loss is {'ner': 1.1421606273650724}\n",
            "\n",
            " iteration 11 , loss is {'ner': 1.9436384234847885}\n",
            "\n",
            " iteration 11 , loss is {'ner': 4.460222164196001}\n",
            "\n",
            " iteration 11 , loss is {'ner': 4.460266660511415}\n",
            "\n",
            " iteration 11 , loss is {'ner': 5.59955570672242}\n",
            "\n",
            " iteration 12 , loss is {'ner': 1.071404035414389}\n",
            "\n",
            " iteration 12 , loss is {'ner': 4.031623411228111}\n",
            "\n",
            " iteration 12 , loss is {'ner': 4.274111732563301}\n",
            "\n",
            " iteration 12 , loss is {'ner': 4.353356331777891}\n",
            "\n",
            " iteration 12 , loss is {'ner': 4.353676625240044}\n",
            "\n",
            " iteration 13 , loss is {'ner': 1.5178358885037824}\n",
            "\n",
            " iteration 13 , loss is {'ner': 1.5210192024019697}\n",
            "\n",
            " iteration 13 , loss is {'ner': 2.719711785700948}\n",
            "\n",
            " iteration 13 , loss is {'ner': 2.753571967666571}\n",
            "\n",
            " iteration 13 , loss is {'ner': 3.5034278032748833}\n",
            "\n",
            " iteration 14 , loss is {'ner': 0.8430099305468031}\n",
            "\n",
            " iteration 14 , loss is {'ner': 1.507152967085309}\n",
            "\n",
            " iteration 14 , loss is {'ner': 3.510823295299476}\n",
            "\n",
            " iteration 14 , loss is {'ner': 4.044753241095888}\n",
            "\n",
            " iteration 14 , loss is {'ner': 5.04338618607784}\n",
            "\n",
            " iteration 15 , loss is {'ner': 1.0352464193338164}\n",
            "\n",
            " iteration 15 , loss is {'ner': 2.728797128659087}\n",
            "\n",
            " iteration 15 , loss is {'ner': 2.927259312128106}\n",
            "\n",
            " iteration 15 , loss is {'ner': 2.9283646517641895}\n",
            "\n",
            " iteration 15 , loss is {'ner': 4.475584751460683}\n",
            "\n",
            " iteration 16 , loss is {'ner': 0.9850246786641037}\n",
            "\n",
            " iteration 16 , loss is {'ner': 2.2285664869994832}\n",
            "\n",
            " iteration 16 , loss is {'ner': 2.230469733968363}\n",
            "\n",
            " iteration 16 , loss is {'ner': 2.2343606228473902}\n",
            "\n",
            " iteration 16 , loss is {'ner': 3.690193914167953}\n",
            "\n",
            " iteration 17 , loss is {'ner': 1.2737835853130832}\n",
            "\n",
            " iteration 17 , loss is {'ner': 1.2938964902050447}\n",
            "\n",
            " iteration 17 , loss is {'ner': 2.133490616459129}\n",
            "\n",
            " iteration 17 , loss is {'ner': 2.1336739194147594}\n",
            "\n",
            " iteration 17 , loss is {'ner': 2.26039361165699}\n",
            "\n",
            " iteration 18 , loss is {'ner': 0.0027542349020341916}\n",
            "\n",
            " iteration 18 , loss is {'ner': 1.7100552695347433}\n",
            "\n",
            " iteration 18 , loss is {'ner': 2.277900232714895}\n",
            "\n",
            " iteration 18 , loss is {'ner': 4.101042106913436}\n",
            "\n",
            " iteration 18 , loss is {'ner': 4.430597488085816}\n",
            "\n",
            " iteration 19 , loss is {'ner': 0.21738846095931486}\n",
            "\n",
            " iteration 19 , loss is {'ner': 2.6344591416238803}\n",
            "\n",
            " iteration 19 , loss is {'ner': 2.634466930309461}\n",
            "\n",
            " iteration 19 , loss is {'ner': 2.634552641757605}\n",
            "\n",
            " iteration 19 , loss is {'ner': 2.636760960432659}\n",
            "\n",
            " iteration 20 , loss is {'ner': 1.2936527136448925e-05}\n",
            "\n",
            " iteration 20 , loss is {'ner': 0.9464553717781296}\n",
            "\n",
            " iteration 20 , loss is {'ner': 3.1523756906763394}\n",
            "\n",
            " iteration 20 , loss is {'ner': 3.152579066250129}\n",
            "\n",
            " iteration 20 , loss is {'ner': 3.154743658243299}\n",
            "\n",
            " iteration 21 , loss is {'ner': 0.8269764979799767}\n",
            "\n",
            " iteration 21 , loss is {'ner': 1.121786332146462}\n",
            "\n",
            " iteration 21 , loss is {'ner': 1.1563460500401406}\n",
            "\n",
            " iteration 21 , loss is {'ner': 2.908296938067784}\n",
            "\n",
            " iteration 21 , loss is {'ner': 2.90830591352246}\n",
            "\n",
            " iteration 22 , loss is {'ner': 0.030839818521428697}\n",
            "\n",
            " iteration 22 , loss is {'ner': 0.6979986312820925}\n",
            "\n",
            " iteration 22 , loss is {'ner': 2.2191967348846813}\n",
            "\n",
            " iteration 22 , loss is {'ner': 2.238159108831694}\n",
            "\n",
            " iteration 22 , loss is {'ner': 2.2381594392996034}\n",
            "\n",
            " iteration 23 , loss is {'ner': 1.2054511473046266e-06}\n",
            "\n",
            " iteration 23 , loss is {'ner': 3.3438617963474175e-06}\n",
            "\n",
            " iteration 23 , loss is {'ner': 0.055370374406746664}\n",
            "\n",
            " iteration 23 , loss is {'ner': 0.05540600981100979}\n",
            "\n",
            " iteration 23 , loss is {'ner': 0.2175630878281995}\n",
            "\n",
            " iteration 24 , loss is {'ner': 0.00011730077911931147}\n",
            "\n",
            " iteration 24 , loss is {'ner': 0.6934544601461472}\n",
            "\n",
            " iteration 24 , loss is {'ner': 0.6934546863545699}\n",
            "\n",
            " iteration 24 , loss is {'ner': 0.6934547502868067}\n",
            "\n",
            " iteration 24 , loss is {'ner': 1.1882874771015137}\n",
            "\n",
            " iteration 25 , loss is {'ner': 0.0007198368218467033}\n",
            "\n",
            " iteration 25 , loss is {'ner': 1.65205800329605}\n",
            "\n",
            " iteration 25 , loss is {'ner': 3.1417760222303066}\n",
            "\n",
            " iteration 25 , loss is {'ner': 3.141776035090575}\n",
            "\n",
            " iteration 25 , loss is {'ner': 3.8462457795360745}\n",
            "\n",
            " iteration 26 , loss is {'ner': 0.0003914989232484123}\n",
            "\n",
            " iteration 26 , loss is {'ner': 0.13918916570904774}\n",
            "\n",
            " iteration 26 , loss is {'ner': 0.8322258869367788}\n",
            "\n",
            " iteration 26 , loss is {'ner': 0.8322632108316391}\n",
            "\n",
            " iteration 26 , loss is {'ner': 0.8323394090716981}\n",
            "\n",
            " iteration 27 , loss is {'ner': 0.04805403196835305}\n",
            "\n",
            " iteration 27 , loss is {'ner': 0.048281240924756094}\n",
            "\n",
            " iteration 27 , loss is {'ner': 0.08251736877747821}\n",
            "\n",
            " iteration 27 , loss is {'ner': 0.08422555146736782}\n",
            "\n",
            " iteration 27 , loss is {'ner': 0.08422568044844989}\n",
            "\n",
            " iteration 28 , loss is {'ner': 0.00234988626890947}\n",
            "\n",
            " iteration 28 , loss is {'ner': 1.9929418368378282}\n",
            "\n",
            " iteration 28 , loss is {'ner': 3.0577818022880696}\n",
            "\n",
            " iteration 28 , loss is {'ner': 3.2372243788772357}\n",
            "\n",
            " iteration 28 , loss is {'ner': 3.2415598605828726}\n",
            "\n",
            " iteration 29 , loss is {'ner': 1.5303497413743372e-07}\n",
            "\n",
            " iteration 29 , loss is {'ner': 0.0001501617896646091}\n",
            "\n",
            " iteration 29 , loss is {'ner': 0.007468367390470037}\n",
            "\n",
            " iteration 29 , loss is {'ner': 0.007468495481955261}\n",
            "\n",
            " iteration 29 , loss is {'ner': 0.09789947203048102}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS7FnKD_92XU"
      },
      "source": [
        "**Let’s predict on new texts the model has not seen**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0sZERE8jmyU",
        "outputId": "b598daf9-e7c9-4e30-f04a-356ccc68c399"
      },
      "source": [
        "#making predictions after training\n",
        "\n",
        "doc=nlp(\"I was driving a Alto\")\n",
        "print(\"\\n entities ,\",[ (ent.text , ent.label_) for ent in doc.ents])\n",
        "\n",
        "doc = nlp(\"Fridge can be ordered in FlipKart\" )\n",
        "print(\"\\n entities ,\",[ (ent.text , ent.label_) for ent in doc.ents])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " entities , [('Alto', 'PRODUCT')]\n",
            "\n",
            " entities , [('Fridge', 'PRODUCT'), ('FlipKart', 'ORG')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt1w3t9mj_Gu"
      },
      "source": [
        "#You can observe that even though I didn’t directly train the model to recognize “Alto” as a vehicle name, it has predicted based on the similarity of context.\n",
        "#This is the awesome part of the NER model.\n",
        "\n",
        "#The model does not just memorize the training examples. It should learn from them and be able to generalize it to new examples.\n",
        "\n",
        "#Once you find the performance of the model satisfactory, save the updated model.\n",
        "\n",
        "#You can save it your desired directory through the to_disk command."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}